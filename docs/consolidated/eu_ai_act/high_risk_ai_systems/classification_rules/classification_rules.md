# High-Risk AI Systems: Classification Rules

## Description

The EU AI Act defines a high-risk AI system based on its intended purpose and the potential risks it poses to health, safety, and fundamental rights. There are two main ways an AI system can be classified as high-risk.

## Legal Basis

The classification rules for high-risk AI systems are laid down in **Article 6 of the EU AI Act**.

## Classification Criteria

An AI system is considered high-risk if it meets **both** of the following conditions:

1.  **It is intended to be used as a safety component of a product, or is itself a product, covered by the Union harmonisation legislation listed in Annex I of the Act.**
2.  **The product whose safety component is the AI system, or the AI system itself as a product, is required to undergo a third-party conformity assessment with a view to the placing on the market or the putting into service of that product pursuant to the Union harmonisation legislation listed in Annex I.**

In addition, AI systems listed in **Annex III** of the Act are also considered to be high-risk.

## Annex I: Union Harmonisation Legislation

Annex I lists Union harmonisation legislation for products where AI systems used as safety components would be considered high-risk. This includes legislation for:

*   Machinery
*   Toys
*   Lifts
*   Personal protective equipment
*   Medical devices
*   And others...

## Annex III: High-Risk AI Systems

Annex III lists specific areas and use cases where AI systems are considered high-risk. These include:

*   **Biometrics:** Remote biometric identification systems (excluding narrow verification use), biometric categorisation based on sensitive attributes, and emotion recognition systems.
*   **Critical infrastructure:** AI safety components managing or operating critical digital infrastructure, road traffic, or the supply of water, gas, heating or electricity.
*   **Education and vocational training:** Systems that determine admission or assignment, evaluate learning outcomes, assess the level of education an individual should receive, or monitor prohibited behaviour during tests.
*   **Employment, workers' management and access to self-employment:** Systems driving recruitment and selection (including targeted advertising and screening) or making, supporting or monitoring work-related decisions and performance.
*   **Access to essential services:** Systems used by public authorities to decide on access to essential public services or benefits; systems that evaluate creditworthiness or credit scores; systems that price or assess risk for life and health insurance; and systems that triage or dispatch emergency first-response services.
*   **Law enforcement:** Systems for victim-risk assessments; polygraphs or similar tools; assessments of evidentiary reliability; assessments of offending or re-offending risks, personality traits or past behaviour (beyond pure profiling); and systems that profile natural persons during criminal investigations under Directive (EU) 2016/680.
*   **Migration, asylum and border control management:** Systems used by competent authorities or EU bodies as polygraphs, to assess security, migration or health risks, to support examination of asylum/visa/residence applications (including evidence checks), or to detect, recognise or identify persons other than simple document verification.
*   **Administration of justice and democratic processes:** Systems assisting judicial authorities (or alternative dispute-resolution bodies) with legal research, interpretation and application, and systems intended to influence the outcome of elections or referenda or voting behaviour (excluding purely logistical campaigning tools).

## Derogation for Low-Risk Use Cases

An AI system referred to in Annex III is **not** considered high-risk if it does not pose a significant risk of harm to the health, safety, or fundamental rights of natural persons, including by not materially influencing the outcome of decision-making. This derogation is based on fulfilling one of the following conditions:

*   The AI system is intended to perform a narrow procedural task.
*   The AI system is intended to improve the result of a previously completed human activity.
*   The AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns and is not meant to replace or influence the previously completed human assessment, without proper human review.
*   The AI system is intended to perform a preparatory task to an assessment relevant for the purposes of the use cases listed in Annex III.

However, an AI system that performs profiling of natural persons is **always** considered high-risk.

Providers who conclude that an Annex III system meets the derogation must document their assessment before market placement, register the system in the EU database under Article 49(2), and supply the assessment to competent authorities upon request (Article 6(4)). The Commission will publish practical guidance and examples by 2 February 2026, and may adjust the derogation conditions via delegated acts (Article 6(5)-(8)).
